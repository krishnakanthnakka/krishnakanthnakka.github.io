<!DOCTYPE HTML>
<html lang="en"><head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125946091-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125946091-1');
</script>


  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Krishna Kanth Nakka</title>

  <meta name="author" content="Krishna Kanth Nakka">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Krishna Kanth Nakka</name>
              </p>
              <p>I have defended my PhD thesis in June 2022 from <a href="https://www.epfl.ch/labs/cvlab/"> CVLab</a>, Department of Computer Science, EPFL.
                I'm fortunate to be supervised by <a href="https://people.epfl.ch/cgi-bin/people?id=119864&op=bio&lang=en&cvlang=en">Dr. Mathieu Salzmann</a> and <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Prof. Pascal Fua</a> in the areas of computer vision and deep learning.
              </p>
               <p>
            Before joining EPFL in 2017, I spent two years at <a href="https://research.samsung.com/sri-b"> Samsung Research Bangalore</a> working on mobile camera algorithms. Prior to that, I graduated from the Department of Electrical Engineering
            at <a href="http://www.iitkgp.ac.in/">IIT Kharagpur</a>  in 2015 with a Dual degree (Masters and Bachelors).
            During my undergraduate and graduate years, I interned at the <a href="https://www.ualberta.ca/computing-science/index.html">University of Alberta</a>,
            <a href="https://www.google.com/search?q=cai+uq&oq=CAI+uq&aqs=chrome.0.0i512j0i22i30j0i15i22i30j0i22i30l3j0i390l4.2150j0j4&sourceid=chrome&ie=UTF-8University">the University of Queensland</a>, and <a href="https://www.philips.com/a-w/about/innovation/innovation-hubs/bangalore.html">Philips Research</a>.
            </p>
            <p>
            From Sep 2022, I will be working as Postdoctoral Scientist at  <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Vehicle Intelligence Transportation (VITA) Lab </a> under the supervision of <a href="https://people.epfl.ch/alexandre.alahi?lang=en">Prof. Alexandre Alahi</a>.

              </p>
              <p style="text-align:center">
                <a href="mailto:krishkanth.92@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume_KrishnaKanthNakka.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=g_21RKoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/krishnakanthnakka/">Github</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/krishna-kanth-nakka-b73a4b44/">LinkedIn</a>&nbsp/&nbsp
                <a href="data/Thesis_final_compressed.pdf">Thesis</a> &nbsp/&nbsp
                <a href="data/Public_Defense_pdf_1.pdf">Thesis Slides</a>


              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/bio3.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/bio3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
       My long-term goal is to build robust and interpretable machine learning models for safety and security-critical applications.
Recently, I started to explore uncertainty quantification and reidentification techniques for keypoint-based pose estimation problem.

<br>
<br>
<!--My current research focus at VITA lab is to understand the limitations of deep neural networks against out-of-distribution and adversarial examples and further improve the robustness to domain shits. -->

During my Ph.D., I focused on understanding the limitations of deep neural networks against out-of-distribution and adversarial examples and further improve the robustness to domain shits.
In particular, I explored the topics of interpretable models, transfer-based black-box attacks, attack detection, adversarial defenses, anomaly detection, and evaluation of disentangled representations.

<br>
<br>
<font color="red">[New] </font> Please contact me if you are interested in pursuing a semester project at the VITA lab.

</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!--
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vita.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <papertitle>  A unified framework for keypoint detection, tracking and re-identification for team sport analysis </papertitle>
              </a>
               <br>
              Innosuisse VITA-Dartfish, <a href="https://www.epfl.ch/research/domains/transportation-center/research-overview/others/team-sports-analysis-via-image-based-tracking/">Ongoing project 2022-23</a>
              <br>
              <p> The goal of this project is to improve the tracking of sports players through a unified framework by detecting and tracking the semantic keypoints and further
                leveraging re-identification techniques to improve the performance of long-term tracking especially when the players go out-of-view.
              </p>

            </td>
          </tr>
        -->


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/disentanglement.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
                                          <a href="data/posepaper.pdf">

                <papertitle>Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Preprint</em>, 2022
               <br>
              <a href="data/posepaper.pdf">Paper</a>
              <br>
              <p>Our analyses show that disentanglement in the three state-of-the-art disentangled representation learning frameworks is far from complete,
              and that their pose codes contain significant appearance information</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pami.jpg" alt="teaser" width="240" height="120">
            </td>
            <td width="75%" valign="middle">
                            <a href="data/pami.pdf">

                <papertitle>Universal, Transferable Adversarial Attacks for Visual Object Trackers</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <a href="data/pami.pdf">Paper</a>
              <br>
              <em>Adversarial Robustness Workshop, European Conference on Computer Vision (ECCV)</em>, 2022
              <p>We propose to learn to generate a single perturbation from the
              object template only, that can be added to every search image and still successfully fool the tracker for the entire video. As a
              consequence, the resulting generator outputs perturbations that are quasi-independent of the template, thereby making them universal
              perturbations.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neurips.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/7486cef2522ee03547cfb970a404a874-Abstract.html">
                <papertitle>Learning Transferable Adversarial Perturbations</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Neural Information and Processing Systems (NeurIPS)</em>, 2021
              <br>
              <a href="https://proceedings.neurips.cc/paper/2021/hash/7486cef2522ee03547cfb970a404a874-Abstract.html">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Transferable_Perturbations">code</a>
              <br>
              <p>We show that generators trained with mid-level feature separation loss transfers significantly better in cross-model, cross-domain and cross-task setting</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/accv.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Nakka_Towards_Robust_Fine-grained_Recognition_by_Maximal_Separation_of_Discriminative_Features_ACCV_2020_paper.html">
                <papertitle>Towards Robust Fine-grained Recognition by Maximal Separation of Discriminative Features</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> Asian Conference on Computer Vision (ACCV)</em>, 2020
              <br>
               <a href="https://arxiv.org/abs/2006.06028">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/RobustFineGrained/">code</a> /
              <a href="data/slides_accv.pdf">Slides</a>
              <br>
              <p>We improve the robustness by introducing an attention-based regularization mechanism that maximally separates the latent features of discriminative regions of different classes
              while minimizing the contribution of the non-discriminative regions to the final class prediction.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv2020.jpg" alt="teaser" width="240" height="170">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">
                <papertitle>Indirect Local Attacks for Context-aware Semantic Segmentation Networks</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>European Conference on Computer Vision  (ECCV)</em>, 2020 <strong>[Spotlight]</strong>
              <br>
               <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Indirectlocalattacks/">code</a> /
              <a href="data/slides_eccv.pdf">Slides</a>

              <br>
              <p> We show that the resulting networks are sensitive not only to global attacks, where perturbations affect the entire input image, but also to indirect local attacks
              where perturbations are confined to a small image region that does not overlap with the area that we aim to fool. </p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595">
                <papertitle>Detecting the Unexpected via Image Resynthesis</papertitle>
              </a>
              <br>
              <a href="https://adynathos.net/">Krzysztof Lis</a>, <strong>Krishna Kanth Nakka</strong>,  <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Pascal Fua</a> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> International Conference on Computer Vision (ICCV) </em>, 2019
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a> /
              <a href="data/DetectingTheUnexpected_Poster.pdf">Poster</a>

              <br>
              <p> We rely on the intuition that the network will produce spurious labels in regions depicting unexpected anomaly objects.
              Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image which we detect through  an auxiliary network</p>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccvw.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1901.02229">
                <papertitle>Interpretable BoW Networks for Adversarial Example Detectio</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Explainable and Interpretable AI workshop, ICCV</em>, 2018  <strong>[Oral]</strong>
              <br>
               <a href="https://arxiv.org/abs/1901.02229">arXiv</a> /
              <a href="data/iccvw_slides.pdf">Slides</a>

              <br>
              <p> We build upon the intuition that, while adversarial samples look very similar to real images, to produce incorrect predictions, they should activate
                codewords with a significantly different visual representation.
              We therefore cast the adversarial example detection problem as that of comparing the input image with the most highly activated visual codeword.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bmvc.jpg" alt="teaser" width="240" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1805.05389">
                <papertitle>Deep Attentional Structured Representation Learning for Visual Recognition</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>British Media Vision Conference (BMVC)</em>, 2018
              <br>
               <a href="https://arxiv.org/abs/1805.05389">arXiv</a> /
                              <a href="data/BMVC2018_Poster.pdf">Poster</a>

                             <br>
              <p> we introduce an attentional structured representation learning framework that incorporates an image-specific attention mechanism within the aggregation process. </p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv2016.jpg" alt="teaser" width="240" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1609.07727">
                <papertitle>Deep learning based fence segmentation and removal from an image using a video sequence</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>International Workshop on Video Segmentation, ECCV</em>, 2016 <strong>[Oral]</strong>
              <br>
               <a href="https://arxiv.org/abs/1609.07727">arXiv</a> /
              <a href="data/Slides_ECCV2016.pdf">Slides</a>
              <br>
              <p>  We use  knowledge of spatial locations of fences to subsequently estimate  occlusion-aware optical flow. We then fuse the occluded information from neighbouring frames
              by solving inverse problem of denoising</p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/josaa.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-33-10-1917">
                <papertitle>Detection and removal of fence occlusions in an image using a video of the static/dynamic scene</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Journal of the Optical Society of America A (JOSA A) </em>, 2016
              <br>
               <a href="https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-33-10-1917">arXiv</a> /
              <a href="data/JOSA.pdf">PDF</a>
              <br>
              <p> Our approach of defencing is as follows: (i) detection of spatial locations of fences/occlusions in the frames of the video, (ii) estimation
              of relative motion between the observations, and (iii) data fusion to fill in occluded pixels in the reference image. We assume the de-fenced image as a Markov random
            field and obtain its maximum a posteriori estimate by solving the corresponding inverse problem. </p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acpr.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7486506">
                <papertitle>My camera can see through fences: A deep learning approach for image de-fencing</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Asian Conference on Pattern Recognition (ACPR),  </em>, 2015
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/7486506">arXiv</a> /
              <a href="data/My_camera_can_see_through_fences_A_deep_learning_approach_for_image_de-fencing.pdf">PDF</a> /
              <a href="data/acpr_poster.pdf">Poster</a>
               <p> We propose a semi-automated de-fencing algorithm using a video of the dynamic scene. The inverse problem offence removal is solved using split Bregman
                technique assuming total variation of the de-fenced image as the regularization constraint.
              </p>

              <br>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/globalsip.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7032076">
                <papertitle>3D-to-2D mapping for user interactive segmentation of human leg muscles from MRI data</papertitle>
              </a>
              <br>
              Nilanjan Ray, Satarupa Mukherjee, <strong>Krishna Kanth Nakka</strong>, Scott T. Acton, Silvia S. Blanker
              <br>
              <em>Signal and Information Processing, GlobalSIP</em>, 2014
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/7032076">arXiv</a> /
              <a href="data/3D-to-2D_mapping_for_user_interactive_segmentation_of_human_leg_muscles_from_MRI_data.pdf">PDF</a>
              <br>
              <p>
              We proposing a framework for user interactive segmentation of MRI of human leg muscles built upon the the strategy of bootstrapping with minimal supervision.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nus1.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pubs.rsc.org/en/content/articlepdf/2014/cp/c4cp02172j">
                <papertitle>Non-uniform sampling in EPR: optimizing data acquisition for Hyscore spectroscopy</papertitle>
              </a>
              <br>
               <strong>Krishna Kanth Nakka</strong> Y. A. Tesiram, I. M. Brereton,  M. Mobli and J. R. Harmer
              <br>
              <em>Physical Chemistry Chemical Physics (PCCP)</em>, 2014
              <br>
               <a href="https://pubs.rsc.org/en/content/articlepdf/2014/cp/c4cp02172j">Paper</a> /
                <a href="data/pccp_main.pdf">PDF</a> /
                <a href="data/pccp_supp.pdf">Supp</a>
              <br>
              <p>We show through non-linear sampling scheme with maximum entropy reconstruction technique in HYSCORE, the experimental times can be shortened by
              approximately an order of magnitude as compared to conventional linear sampling with negligible loss of information
              </p>

            </td>
          </tr>
        </tbody>
      </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Scholarships</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>

              <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">MITACS Summer Research Scholarship to conduct research at University of Alberta</a>
              <br>
              <a href="https://science.uq.edu.au/student-support/scholarships/undergraduate-scholarships/uq-summer-research-program">University of Queensland Summer Research Scholarship</a>               to support the intership at <a href="https://cai.centre.uq.edu.au/">Center for Advanced Imaging Institute</a>
              <br>
              <a href="https://www.epfl.ch/education/phd/edic-computer-and-communication-sciences/edic-for-phd-students/">EDIC PhD Fellowship</a> for pursuing first year of doctoral studies at EPFL
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:10px">
              <br>
              <p style="text-align:right;font-size:small;">
                Webpage template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>
</html>
