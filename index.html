<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Krishna Kanth Nakka</title>

  <meta name="author" content="Krishna Kanth Nakka">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Krishna Kanth Nakka</name>
              </p>
              <p>I've successfully defended my PhD thesis in June 2022 from CVLab, Department of Computer Science, EPFL.
                I'm extremely fortunate to be supervised by <a href="https://people.epfl.ch/cgi-bin/people?id=119864&op=bio&lang=en&cvlang=en">Dr. Mathieu Salzmann</a> and <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Prof. Pascal Fua</a> in the areas of computer vision and deep learning.
              </p>
              <p>
            My research focus is to understand the strengths and weaknesses of deep neural networks in safety and security-critical applications.
            My PhD thesis explores the topics of interpretable models, transfer-based black-box attacks, attack detection, adversarial defenses, anomaly detection, and disentangled representations.
              </p>
              <p style="text-align:center">
                <a href="mailto:krishkanth.92@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=g_21RKoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/krishnakanthnakka/">Github</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/krishna-kanth-nakka-b73a4b44/">LinkedIn</a>

              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/bio.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/bio.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                The long-term goal  is to build machine learning algorithms that are more robust, and interpretable.  The site is under construction.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                <papertitle>Understanding Pose and Appearance Disentanglement in 3D Human Pose
Estimation</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Preprint </em>, 2022
              <p>Our analyses show that disentanglement in the three state-of-the-art disentangled
representation learning frameworks if far from complete, and
that their pose codes contain significant appearance information</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                <papertitle>Universal, Transferable Adversarial Attacks for Visual Object Trackers</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Preprint </em>, 2022
              <p>We propose to learn to generate a single perturbation from the
object template only, that can be added to every search image and still successfully fool the tracker for the entire video. As a
consequence, the resulting generator outputs perturbations that are quasi-independent of the template, thereby making them universal
perturbations.</p>

            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                <papertitle>Learning Transferable Adversarial Perturbations</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Neural Information and Processing Systems (NeurIPS)</em>, 2021
              <br>
              <a href="https://proceedings.neurips.cc/paper/2021/hash/7486cef2522ee03547cfb970a404a874-Abstract.html">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Transferable_Perturbations">code</a>
              <br>
              <p>Generators trained with mid-level feature separation loss transfers significantly better in cross-model, cross-domain and cross-task setting</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Nakka_Towards_Robust_Fine-grained_Recognition_by_Maximal_Separation_of_Discriminative_Features_ACCV_2020_paper.html">
                <papertitle>Towards Robust Fine-grained Recognition by Maximal Separation of Discriminative Features</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> Asian Conference on Computer Vision  (ACCV)</em>, 2020
              <br>
               <a href="https://arxiv.org/abs/2006.06028">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/RobustFineGrained/">code</a>
              <br>
              <p>We improve the robustness by introducing an attention-based regularization mechanism that maximally separates the latent features of discriminative regions of different classes
              while minimizing the contribution of the non-discriminative regions to the final class prediction.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">
                <papertitle>Indirect Local Attacks for Context-aware Semantic Segmentation Networks</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>European Conference on Computer Vision  (ECCV)</em>, 2020 <strong>[Spotlight]</strong>
              <br>
               <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Indirectlocalattacks/">code</a>
              <br>
              <p>we show that the resulting networks are sensitive not only to global attacks, where perturbations affect the entire input image, but also to indirect local attacks
              where perturbations are confined to a small image region that does not overlap with the area that we aim to fool. </p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Detecting the Unexpected via Image Resynthesis</papertitle>
              </a>
              <br>
              Krzysztof Lis, <strong>Krishna Kanth Nakka</strong>,  Pascal Fua and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> International Conference on Computer Vision (ICCV) </em>, 2019
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>
              <br>
              <p> We rely on the intuition that the network will produce spurious labels in regions depicting unexpected anomaly objects.
              Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image which we detect through  an auxiliary network</p>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Interpretable BoW Networks for Adversarial Example Detectio</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Explainable and Interpretable AI workshop, ICCV 2019.</em>, 2018
              <br>
               <a href="https://arxiv.org/abs/1901.02229">arXiv</a> /
              <br>
              <p>we build upon the intuition that, while adversarial samples look very similar to real images, to produce incorrect predictions, they should activate codewords with a significantly different visual representation.
              We therefore cast the adversarial example detection problem as that of comparing the input image with the most highly activated visual codeword.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Deep Attentional Structured Representation Learning for Visual Recognition</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>British Media Vision Conference (BMVC)</em>, 2018
              <br>
               <a href="https://arxiv.org/abs/1805.05389">arXiv</a>
                             <br>
              <p> we introduce an attentional structured representation learning framework that incorporates an image-specific attention mechanism within the aggregation process. </p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Deep learning based fence segmentation and removal from an image using a video sequence</papertitle>
              </a>
              <br>
               SankarGanesh Jonna ,  ,<strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>International Workshop on Video Segmentation, ECCV </em>, 2016
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>
              <br>
              <p> </p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Detection and removal of fence occlusions in an image using a video of the
static/dynamic scene</papertitle>
              </a>
              <br>
               SankarGanesh Jonna ,  ,<strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>International Workshop on Video Segmentation, ECCV </em>, 2016
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>
              <br>
              <p> </p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>My camera can see through fences: A deep learning approach for image
de-fencing</papertitle>
              </a>
              <br>
               SankarGanesh Jonna ,  ,<strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Asian Conference on Pattern Recognition (ACPR),  </em>, 2015
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>
                            <p> </p>

              <br>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Towards an Automated Image De-fencing Algorithm Using Sparsity</papertitle>
              </a>
              <br>
               SankarGanesh Jonna ,  ,<strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Asian Conference on Pattern Recognition (ACPR),  </em>, 2015
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>

              <br>
                            <p> </p>


            </td>
          </tr>


            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Towards an Automated Image De-fencing Algorithm Using Sparsity</papertitle>
              </a>
              <br>
               SankarGanesh Jonna ,  ,<strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>International Conference on Computer Vision Theory and Applications, VISAPP </em>, 2015
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>
              <br>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>3D-to-2D mapping for user interactive segmentation of human leg muscles from MRI data</papertitle>
              </a>
              <br>
               SankarGanesh Jonna ,  ,<strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Signal and Information Processing, GlobalSIP</em>, 2014
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>
              <br>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t2.jpg" alt="teaser" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595f">
                <papertitle>Non-uniform sampling in EPR: optimizing data acquisition for Hyscore spectroscopy</papertitle>
              </a>
              <br>
               SankarGanesh Jonna ,  ,<strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Physical Chemistry Chemical Physics</em>, 2014
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a>
              <br>

            </td>
          </tr>






        </tbody>
      </table>







      </td>
    </tr>
  </table>
</body>



</html>
